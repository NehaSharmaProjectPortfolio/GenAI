# Dialogue Summarization from a Pre-Trained Generative AI LLM Model (FLAN-T5)
---
### Project Objective: To study dialogue summarization task using generative AI Pre-Trained LLM Model (FLAN-T5) and impacts of prompt engineering to achieve desired outcomes.
* We will explore the impact of input text on the output of the model
* Then we will further perform prompt engineering to attempt to achieve the desired summarization against the human labelled benchmark. 
* We will then carry out zero shot, one shot, and few shot inferences and observe how it can enhance the generative output of Large Language Models like FLAN-T5 Model
---
### Tools
- Amazon SageMaker :  We used Amazon SageMaker and its studio to load and deploy Gen AI LLM Model for our use case with fully managed infrastructure, tools, and workflows. The Configuration details are (ml.m5.2xlarge)
- Python : We have written python code for loading the FLAN-T5 model and the test datasets from Hugging Face.
         : We installed Pytorch and Hugging Face transformers and datasets
---
### AI Model Detail
- Pre-trained Large Language Model(LLM) FLANT-5 from Hugging Face
- List of avialable models and documentation from Hugging Face can be referred [here](https://huggingface.co/docs/transformers/index)
### Librarires

### Project Steps

### Results

### Limitations

### Reference
